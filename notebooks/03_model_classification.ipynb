{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuYitQT6s6Ag"
      },
      "outputs": [],
      "source": [
        "# 11. CLASSIFICATION MODEL ARCHITECTURES (REVISED: PRE-NORM & EXTERNAL COMPILE)\n",
        "# ==============================================================================\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "\n",
        "print(\"Building Neural Network Architectures (Mode: Classification)...\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODEL 1: ResNet-MLP (Deep Residual Network) - CLASSIFICATION\n",
        "# REVISION: Post-Norm -> Pre-Norm & Remove Internal Compile\n",
        "# ------------------------------------------------------------------------------\n",
        "def build_mlp_classifier(input_dim, output_dim=5, width=256, depth=4, dropout_rate=0.2):\n",
        "    inputs = layers.Input(shape=(input_dim,), name='input_features')\n",
        "\n",
        "    # Project to initial dimension\n",
        "    x = layers.Dense(width, activation='linear')(inputs)\n",
        "\n",
        "    # --- Residual Blocks Loop (Pre-Norm Style) ---\n",
        "    for i in range(depth):\n",
        "        shortcut = x # Save main path (Skip Connection)\n",
        "\n",
        "        # 1. Normalize first (Pre-Norm)\n",
        "        # This stabilizes gradients in deep networks\n",
        "        x_norm = layers.LayerNormalization()(x)\n",
        "\n",
        "        # 2. Transformation Block (Dense -> Dropout -> Dense)\n",
        "        branch = layers.Dense(width, activation='gelu')(x_norm)\n",
        "        branch = layers.Dropout(dropout_rate)(branch)\n",
        "        branch = layers.Dense(width, activation='linear')(branch)\n",
        "\n",
        "        # 3. Merge back (Add)\n",
        "        x = layers.Add()([shortcut, branch])\n",
        "\n",
        "    # Final Norm (Mandatory at the end of Pre-Norm networks)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "\n",
        "    # Head\n",
        "    x = layers.Dense(64, activation='gelu')(x)\n",
        "\n",
        "    # Output Layer\n",
        "    outputs = layers.Dense(output_dim, activation='softmax', name='class_output')\n",
        "\n",
        "    # NOTE: model.compile is REMOVED here\n",
        "    # This allows us to inject external Schedulers/Optimizers later during the training loop.\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name=\"ResNet_MLP_Classifier\")\n",
        "    return model\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODEL 2: TABULAR TRANSFORMER - CLASSIFICATION\n",
        "# REVISION: GAP -> Flatten & Remove Internal Compile\n",
        "# ------------------------------------------------------------------------------\n",
        "def build_transformer_classifier(input_dim, output_dim=5, embed_dim=64, num_heads=4, num_blocks=3, dropout=0.1):\n",
        "    inputs = layers.Input(shape=(input_dim,), name='input_features')\n",
        "\n",
        "    # 1. Feature Tokenizer\n",
        "    # Treats each feature as a token (Simple embedding via Conv1D)\n",
        "    x = layers.Reshape((input_dim, 1))(inputs)\n",
        "    x = layers.Conv1D(embed_dim, kernel_size=1, activation=None)(x)\n",
        "\n",
        "    # 2. Transformer Blocks (Pre-Norm Style)\n",
        "    for _ in range(num_blocks):\n",
        "        # Attention Block\n",
        "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=dropout)(x_norm, x_norm)\n",
        "        x = layers.Add()([x, attn_output])\n",
        "\n",
        "        # Feed Forward Block\n",
        "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn = models.Sequential([\n",
        "            layers.Dense(embed_dim * 2, activation='gelu'),\n",
        "            layers.Dropout(dropout),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        x = layers.Add()([x, ffn(x_norm)])\n",
        "\n",
        "    # 3. Prediction Head (REVISION: Flatten)\n",
        "    # Replaced GlobalAveragePooling with Flatten to preserve specific feature positions\n",
        "    # (Important for tabular data where column order is fixed)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    x = layers.Dense(64, activation='gelu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    # Output\n",
        "    outputs = layers.Dense(output_dim, activation='softmax', name='class_output')\n",
        "\n",
        "    # NOTE: model.compile is REMOVED here as well\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name=\"Transformer_Classifier\")\n",
        "    return model\n",
        "\n",
        "print(\"Classification Architectures Ready (Pre-Norm, Flatten, & External Optimizer Ready).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. CLASSIFICATION TRAINING: PRE-NORM RESNET-MLP (REVISED: STABILITY & SCHEDULER)\n",
        "# ==============================================================================\n",
        "try:\n",
        "    import keras_tuner as kt\n",
        "except ImportError:\n",
        "    !pip install keras_tuner -q\n",
        "    import keras_tuner as kt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os, shutil\n",
        "\n",
        "# Set Scientific Plot Style\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['axes.grid'] = False\n",
        "\n",
        "print(\"Starting Pre-Norm ResNet Tuning Pipeline (Mode: Stable & Warmup)...\")\n",
        "\n",
        "# --- 1. LOAD DATA ---\n",
        "if 'df_classification' in locals():\n",
        "    df_use = df_classification.copy()\n",
        "else:\n",
        "    try:\n",
        "        df_use = pd.read_parquet(\"df_cls_final.parquet\")\n",
        "        print(\"   -> Loaded from Parquet.\")\n",
        "    except:\n",
        "        raise ValueError(\"Error: df_classification not found!\")\n",
        "\n",
        "input_cols = [\n",
        "    'bp_rp0', 'bp_g', 'g_rp',\n",
        "    'abs_G0', 'parallax', 'ruwe',\n",
        "    'l_norm', 'b_norm'\n",
        "]\n",
        "\n",
        "# Check columns\n",
        "available_cols = [c for c in input_cols if c in df_use.columns]\n",
        "target_col = 'label_code'\n",
        "\n",
        "X = df_use[available_cols].values\n",
        "y = df_use[target_col].values\n",
        "\n",
        "# --- FEATURE CORRELATION CHECK ---\n",
        "print(\"Generating Feature Correlation Matrix...\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = df_use[available_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scaling (QuantileTransformer is best for non-Gaussian Astro data)\n",
        "print(\"   -> Scaling Data (QuantileTransformer)...\")\n",
        "scaler = QuantileTransformer(output_distribution='normal')\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Class Weights (To handle any remaining imbalance)\n",
        "from sklearn.utils import class_weight\n",
        "classes = np.unique(y_train)\n",
        "weights = class_weight.compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "class_weight_dict = dict(zip(classes, weights))\n",
        "print(\"Class Weights:\", class_weight_dict)\n",
        "\n",
        "# --- SCHEDULER CONFIGURATION ---\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS_SEARCH = 20\n",
        "EPOCHS_FINAL = 50\n",
        "\n",
        "# --- PRE-NORM MODEL DEFINITION (CORE ARCHITECTURE) ---\n",
        "def build_resnet_prenorm_tuner(hp):\n",
        "    inputs = layers.Input(shape=(X_train.shape[1],))\n",
        "\n",
        "    # Hyperparameters\n",
        "    width = hp.Int('width', min_value=128, max_value=256, step=64)\n",
        "    dropout_rate = hp.Float('dropout_rate', 0.2, 0.5, step=0.1)\n",
        "\n",
        "    # Initial Linear Projection\n",
        "    x = layers.Dense(width, activation='linear')(inputs)\n",
        "\n",
        "    # --- RESIDUAL BLOCKS (PRE-NORM STYLE) ---\n",
        "    # Concept: Input -> [Norm -> Dense -> Dropout -> Dense] + Input\n",
        "    # Pre-Norm allows smoother gradient flow in deeper networks.\n",
        "    for i in range(hp.Int('num_blocks', 1, 3)):\n",
        "        # 1. Save Identity (Skip Connection)\n",
        "        shortcut = x\n",
        "\n",
        "        # 2. Normalization FIRST (Pre-Norm)\n",
        "        x_norm = layers.LayerNormalization()(x)\n",
        "\n",
        "        # 3. Transformation Branch\n",
        "        branch = layers.Dense(width, activation='gelu')(x_norm)\n",
        "        branch = layers.Dropout(dropout_rate)(branch)\n",
        "        branch = layers.Dense(width, activation='linear')(branch) # Linear ensures additivity\n",
        "\n",
        "        # 4. Add (Residual Connection)\n",
        "        x = layers.Add()([shortcut, branch])\n",
        "\n",
        "    # --- OUTPUT HEAD ---\n",
        "    x = layers.LayerNormalization()(x) # Final Norm is mandatory for Pre-Norm\n",
        "    x = layers.Dense(64, activation='gelu')(x)\n",
        "    outputs = layers.Dense(5, activation='softmax')(x)\n",
        "\n",
        "    # --- OPTIMIZER WITH WARMUP & COSINE DECAY ---\n",
        "    steps_per_epoch = len(X_train_scaled) // BATCH_SIZE\n",
        "    total_steps = steps_per_epoch * EPOCHS_SEARCH\n",
        "\n",
        "    lr_max = hp.Choice('learning_rate', [1e-3, 5e-4])\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=1e-5,      # Start low\n",
        "        decay_steps=total_steps,\n",
        "        alpha=0.01,\n",
        "        warmup_target=lr_max,            # Ramp up to this\n",
        "        warmup_steps=int(0.1 * total_steps)\n",
        "    )\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4),\n",
        "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# --- TUNING EXECUTION ---\n",
        "if os.path.exists('tuning_dir/gaia_resnet_v2'): shutil.rmtree('tuning_dir/gaia_resnet_v2')\n",
        "\n",
        "tuner = kt.Hyperband(build_resnet_prenorm_tuner,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=EPOCHS_SEARCH,\n",
        "                     factor=3,\n",
        "                     directory='tuning_dir',\n",
        "                     project_name='gaia_resnet_v2')\n",
        "\n",
        "print(\"\\nSearching for Best Hyperparameters...\")\n",
        "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "tuner.search(X_train_scaled, y_train,\n",
        "             epochs=EPOCHS_SEARCH,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[stop_early],\n",
        "             batch_size=BATCH_SIZE,\n",
        "             class_weight=class_weight_dict,\n",
        "             verbose=1)\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Width: {best_hps.get('width')}, Best LR: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "# --- FINAL RETRAINING (FULL SCHEDULER) ---\n",
        "print(\"\\nRetraining Best Model (Pre-Norm)...\")\n",
        "\n",
        "# Rebuild model\n",
        "model_final = build_resnet_prenorm_tuner(best_hps)\n",
        "\n",
        "# Recalculate Scheduler for Final Epochs\n",
        "total_steps_final = (len(X_train_scaled) // BATCH_SIZE) * EPOCHS_FINAL\n",
        "lr_final = best_hps.get('learning_rate')\n",
        "\n",
        "new_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=1e-6,\n",
        "    decay_steps=total_steps_final,\n",
        "    alpha=0.01,\n",
        "    warmup_target=lr_final,\n",
        "    warmup_steps=int(0.1 * total_steps_final)\n",
        ")\n",
        "\n",
        "# Recompile\n",
        "model_final.compile(optimizer=optimizers.AdamW(learning_rate=new_schedule, weight_decay=1e-4),\n",
        "                    loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model_final.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=EPOCHS_FINAL,\n",
        "    validation_split=0.2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- EVALUATION ---\n",
        "# A. Learning Curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss Curve (Pre-Norm)')\n",
        "plt.xlabel('Epochs'); plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epochs'); plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('learning_curves_prenorm.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# B. Classification Report & Matrix\n",
        "y_pred_probs = model_final.predict(X_test_scaled)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Standard English Labels\n",
        "class_labels = ['Main Sequence', 'Sub-Giant', 'Red Giant', 'Supergiant', 'White Dwarf']\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_labels))\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "# Normalize\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "            xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.title('Normalized Confusion Matrix (Pre-Norm ResNet)')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_prenorm.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "model_final.save('best_resnet_prenorm_classifier.keras')\n",
        "print(\"Model saved: best_resnet_prenorm_classifier.keras\")"
      ],
      "metadata": {
        "id": "TKrJTCwctjpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. TRANSFORMER CLASSIFICATION TRAINING (REVISED: PRE-NORM, WARMUP & STABILITY)\n",
        "# ==============================================================================\n",
        "print(\"Starting FT-Transformer Pipeline (Mode: Pre-Norm & Warmup)...\")\n",
        "\n",
        "# --- DATA PREPARATION ---\n",
        "if 'df_classification' in locals():\n",
        "    df_use = df_classification.copy()\n",
        "else:\n",
        "    try:\n",
        "        df_use = pd.read_parquet(\"df_cls_final.parquet\")\n",
        "    except:\n",
        "        raise ValueError(\"Error: Data not found! Please run Preprocessing first.\")\n",
        "\n",
        "input_cols = ['bp_rp0', 'bp_g', 'g_rp', 'abs_G0', 'parallax', 'ruwe', 'l_norm', 'b_norm']\n",
        "X = df_use[input_cols].values\n",
        "y = df_use['label_code'].values\n",
        "\n",
        "# --- [VISUALIZATION] FEATURE CORRELATION (TRANSFORMER) ---\n",
        "print(\"Generating Feature Correlation Matrix (Transformer)...\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = df_use[input_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
        "plt.title(\"Feature Correlation Matrix (Transformer)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('transformer_feature_correlation.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "scaler = QuantileTransformer(output_distribution='normal')\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "NUM_FEATURES = X_train_scaled.shape[1]\n",
        "NUM_CLASSES = 5\n",
        "classes = np.unique(y_train)\n",
        "weights = class_weight.compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "class_weight_dict = dict(zip(classes, weights))\n",
        "print(\"Class Weights:\", class_weight_dict)\n",
        "\n",
        "# --- SCHEDULER CONFIGURATION ---\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS_SEARCH = 15\n",
        "EPOCHS_FINAL = 50\n",
        "\n",
        "# --- FT-TRANSFORMER ARCHITECTURE (REVISED: PRE-NORM) ---\n",
        "def build_ft_transformer_prenorm(hp):\n",
        "    inputs = layers.Input(shape=(NUM_FEATURES,))\n",
        "\n",
        "    # Feature Tokenizer (Simple Linear Embedding per feature)\n",
        "    x = layers.Reshape((NUM_FEATURES, 1))(inputs)\n",
        "    embed_dim = hp.Int('embed_dim', min_value=32, max_value=64, step=32)\n",
        "    x = layers.Conv1D(filters=embed_dim, kernel_size=1, activation=None)(x)\n",
        "\n",
        "    # --- Transformer Blocks (Pre-Norm Style) ---\n",
        "    # Pre-Norm is crucial for stability: Norm -> Attention -> Add\n",
        "    for i in range(hp.Int('num_blocks', 1, 3)):\n",
        "        # 1. Attention Block\n",
        "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x) # Normalize FIRST\n",
        "\n",
        "        num_heads = hp.Int(f'num_heads_{i}', 2, 4, step=2)\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads,\n",
        "            dropout=hp.Float(f'attn_dropout_{i}', 0.0, 0.2, step=0.1)\n",
        "        )(x_norm, x_norm)\n",
        "\n",
        "        x = layers.Add()([x, attn_output]) # Skip Connection\n",
        "\n",
        "        # 2. Feed Forward Block\n",
        "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x) # Normalize FIRST\n",
        "\n",
        "        ff_dim = embed_dim * 2\n",
        "        ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"gelu\"),\n",
        "            layers.Dropout(hp.Float(f'ffn_dropout_{i}', 0.0, 0.2, step=0.1)),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        ffn_output = ffn(x_norm)\n",
        "\n",
        "        x = layers.Add()([x, ffn_output]) # Skip Connection\n",
        "\n",
        "    # --- Head (Flatten & Final Norm) ---\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x) # Final Norm is mandatory\n",
        "    x = layers.Flatten()(x) # Flatten preserves feature positional info\n",
        "\n",
        "    x = layers.Dense(64, activation='gelu')(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "    # --- Optimizer with Warmup & Cosine Decay ---\n",
        "    # Transformer needs warmup to avoid early divergence\n",
        "    steps_per_epoch = len(X_train_scaled) // BATCH_SIZE\n",
        "    total_steps = steps_per_epoch * EPOCHS_SEARCH\n",
        "\n",
        "    lr_max = hp.Choice('learning_rate', values=[1e-3, 5e-4])\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=1e-5,      # Start very small\n",
        "        decay_steps=total_steps,\n",
        "        alpha=0.01,\n",
        "        warmup_target=lr_max,            # Ramp up to this target\n",
        "        warmup_steps=int(0.1 * total_steps)\n",
        "    )\n",
        "\n",
        "    weight_decay = hp.Choice('weight_decay', values=[1e-4, 1e-5])\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=optimizers.AdamW(learning_rate=lr_schedule, weight_decay=weight_decay),\n",
        "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# --- TUNING EXECUTION ---\n",
        "if os.path.exists('tuning_dir/gaia_transformer_v2'): shutil.rmtree('tuning_dir/gaia_transformer_v2')\n",
        "\n",
        "tuner = kt.Hyperband(build_ft_transformer_prenorm,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=EPOCHS_SEARCH,\n",
        "                     factor=3,\n",
        "                     directory='tuning_dir',\n",
        "                     project_name='gaia_transformer_v2',\n",
        "                     overwrite=True)\n",
        "\n",
        "print(\"\\nTuning FT-Transformer (Pre-Norm)...\")\n",
        "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "tuner.search(X_train_scaled, y_train,\n",
        "             epochs=EPOCHS_SEARCH,\n",
        "             batch_size=BATCH_SIZE,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[stop_early],\n",
        "             class_weight=class_weight_dict,\n",
        "             verbose=1)\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# --- FINAL TRAINING (FULL SCHEDULER) ---\n",
        "print(\"\\nTraining Final Model (Pre-Norm + Full Warmup)...\")\n",
        "\n",
        "# Rebuild manually to reset weights\n",
        "best_transformer = build_ft_transformer_prenorm(best_hps)\n",
        "\n",
        "# Update Scheduler for full 50 epochs\n",
        "total_steps_final = (len(X_train_scaled) // BATCH_SIZE) * EPOCHS_FINAL\n",
        "lr_final_val = best_hps.get('learning_rate')\n",
        "\n",
        "new_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=1e-5,\n",
        "    decay_steps=total_steps_final,\n",
        "    alpha=0.01,\n",
        "    warmup_target=lr_final_val,\n",
        "    warmup_steps=int(0.1 * total_steps_final)\n",
        ")\n",
        "\n",
        "weight_decay_final = best_hps.get('weight_decay')\n",
        "\n",
        "# Recompile\n",
        "best_transformer.compile(optimizer=optimizers.AdamW(learning_rate=new_schedule, weight_decay=weight_decay_final),\n",
        "                         loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = best_transformer.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=EPOCHS_FINAL,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- EVALUATION & VISUALIZATION ---\n",
        "print(\"\\nTransformer Evaluation:\")\n",
        "loss, acc = best_transformer.evaluate(X_test_scaled, y_test, batch_size=512)\n",
        "print(f\"   Accuracy: {acc:.2%}\")\n",
        "\n",
        "# A. Learning Curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss Curve (Pre-Norm)')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('transformer_learning_curves_prenorm.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# B. Confusion Matrix\n",
        "y_pred = np.argmax(best_transformer.predict(X_test_scaled, batch_size=512), axis=1)\n",
        "labels = ['Main Sequence', 'Sub-Giant', 'Red Giant', 'Supergiant', 'White Dwarf']\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.title('Normalized Confusion Matrix - Pre-Norm Transformer')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('transformer_confusion_matrix_prenorm.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=labels))\n",
        "best_transformer.save(\"best_transformer_classifier_prenorm.keras\")\n",
        "print(\"Model saved: best_transformer_classifier_prenorm.keras\")"
      ],
      "metadata": {
        "id": "zrMZtTY3u1yO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}