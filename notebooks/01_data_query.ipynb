{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxzWG56mAA-"
      },
      "outputs": [],
      "source": [
        "# 0. INSTALLATION AND LIBRARY IMPORT\n",
        "# ==============================================================================\n",
        "\n",
        "!pip install astroquery -q\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn tensorflow -q\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from astroquery.gaia import Gaia\n",
        "from astroquery.vizier import Vizier\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, Reshape,\n",
        "    LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, r2_score, mean_absolute_error\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "Gaia.MAIN_GAIA_TABLE = \"gaiadr3.gaia_source\"\n",
        "Gaia.ROW_LIMIT = -1\n",
        "Vizier.ROW_LIMIT = -1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ONCE YOU HAVE THE CSV DATA, RUN THIS PART\n",
        "# ==============================================================================\n",
        "# JUMP-START: LOAD DATASET WITHOUT RE-QUERY\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"Starting session recovery (Recovery Mode)...\")\n",
        "\n",
        "# I often use Kaggle, so if you use Kaggle, you just need to change this part.\n",
        "\n",
        "# Change the folder name\n",
        "file_path = \"/kaggle/input/final-master-dataset/FINAL_MASTER_DATASET.csv\"\n",
        "\n",
        "# 2. Check and Load\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"File found: {file_path}\")\n",
        "    print(\"Reading the CSV file, please wait a moment...\")\n",
        "    df_gabungan = pd.read_csv(file_path)\n",
        "\n",
        "    print(f\"Data reloaded successfully.\")\n",
        "    print(f\"Number of rows: {len(df_gabungan):,}\")\n",
        "    print(f\"Columns available: {list(df_gabungan.columns[:5])}...\")\n",
        "\n",
        "    # Quick check if Supergiant data exists\n",
        "    if 'dataset_source' in df_gabungan.columns:\n",
        "        print(\"\\nData Source Check:\")\n",
        "        print(df_gabungan['dataset_source'].value_counts())\n",
        "\n",
        "else:\n",
        "    print(f\"File not found at: {file_path}\")\n",
        "    print(\"Tips: If you have downloaded the file to your laptop, please use 'Add Data' -> 'Upload Dataset' at the top right.\")"
      ],
      "metadata": {
        "id": "yQjthukAmSXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. QUERY FOR MAIN SEQUENCE AND SUBGIANT STARS\n",
        "\n",
        "print(\"Fetching main data (Main Sequence, Subgiant, Giant)...\")\n",
        "\n",
        "query_utama_final = \"\"\"\n",
        "SELECT\n",
        "    g.source_id, g.ra, g.dec,\n",
        "    g.l, g.b,\n",
        "    g.parallax, g.parallax_over_error,\n",
        "    g.pmra, g.pmdec,\n",
        "    g.phot_g_mean_mag, g.phot_bp_mean_mag, g.phot_rp_mean_mag,\n",
        "    g.phot_bp_rp_excess_factor,\n",
        "    g.ruwe, g.phot_variable_flag,\n",
        "\n",
        "    -- FLAME & GSP-Phot PARAMETERS\n",
        "    df.teff_gspphot, df.logg_gspphot, df.mh_gspphot,\n",
        "    df.ag_gspphot, df.ebpminrp_gspphot,\n",
        "    df.mg_gspphot,\n",
        "    df.mass_flame, df.age_flame, df.evolstage_flame,\n",
        "    df.flags_flame\n",
        "FROM\n",
        "    gaiadr3.gaia_source AS g\n",
        "JOIN\n",
        "    gaiadr3.astrophysical_parameters AS df\n",
        "    ON g.source_id = df.source_id\n",
        "WHERE\n",
        "    g.parallax > 0.5 AND g.parallax_over_error > 10\n",
        "    AND g.phot_g_mean_mag < 18\n",
        "    AND g.phot_bp_mean_mag IS NOT NULL\n",
        "    AND g.phot_rp_mean_mag IS NOT NULL\n",
        "    AND g.phot_g_mean_mag < 18\n",
        "    AND g.ruwe < 1.4\n",
        "    AND df.flags_flame LIKE '0%'\n",
        "    AND df.evolstage_flame IS NOT NULL\n",
        "    AND df.mass_flame IS NOT NULL\n",
        "    AND df.age_flame IS NOT NULL\n",
        "    AND df.teff_gspphot BETWEEN 2500 AND 50000\n",
        "    AND df.logg_gspphot BETWEEN -0.5 AND 6.0\n",
        "    AND df.mh_gspphot > -5\n",
        "    AND df.mg_gspphot BETWEEN -5 AND 15\n",
        "    AND df.lum_flame IS NOT NULL\n",
        "    AND g.random_index < 50000000\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    job_utama = Gaia.launch_job_async(query_utama_final)\n",
        "    df_utama = job_utama.get_results().to_pandas()\n",
        "    print(f\"Main data acquisition complete. Received {len(df_utama)} rows.\")\n",
        "\n",
        "    if 'l' in df_utama.columns:\n",
        "        print(\" -> Galactic columns (l, b) successfully retrieved.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred: {e}\")"
      ],
      "metadata": {
        "id": "Mmq1ipLXnYhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. QUERY FOR GIANT STARS\n",
        "\n",
        "print(\"[Additional Query] Fetching SPECIFIC GIANT DATA...\")\n",
        "\n",
        "query_giant = \"\"\"\n",
        "SELECT\n",
        "    g.source_id, g.ra, g.dec,\n",
        "    g.l, g.b,\n",
        "    g.parallax, g.parallax_over_error,\n",
        "    g.pmra, g.pmdec,\n",
        "    g.phot_g_mean_mag, g.phot_bp_mean_mag, g.phot_rp_mean_mag,\n",
        "    g.phot_bp_rp_excess_factor,\n",
        "    g.ruwe, g.phot_variable_flag,\n",
        "\n",
        "    -- FLAME PARAMETERS\n",
        "    df.teff_gspphot, df.logg_gspphot, df.mh_gspphot,\n",
        "    df.ag_gspphot, df.ebpminrp_gspphot,\n",
        "    df.mg_gspphot,\n",
        "    df.mass_flame, df.age_flame, df.evolstage_flame,\n",
        "    df.flags_flame\n",
        "FROM\n",
        "    gaiadr3.gaia_source AS g\n",
        "JOIN\n",
        "    gaiadr3.astrophysical_parameters AS df\n",
        "    ON g.source_id = df.source_id\n",
        "WHERE\n",
        "    g.parallax > 0.1             -- Minimum distance (can be further than MS)\n",
        "    AND g.parallax_over_error > 5\n",
        "    AND g.phot_g_mean_mag < 18.5 -- Slightly dimmer magnitude allowed\n",
        "    AND g.phot_bp_mean_mag IS NOT NULL\n",
        "    AND g.phot_rp_mean_mag IS NOT NULL\n",
        "\n",
        "    -- RELAXED FILTER SPECIFIC FOR GIANTS\n",
        "    AND g.ruwe < 3.0             -- Giants have higher astrometric noise, so RUWE < 3.0 is acceptable\n",
        "    -- REMOVED 'flags_flame' filter so 'difficult' Giants are included\n",
        "\n",
        "    -- ONLY FETCH GIANTS (Base RGB to AGB)\n",
        "    AND df.evolstage_flame BETWEEN 490 AND 1290\n",
        "\n",
        "    -- Ensure Target Exists\n",
        "    AND df.mass_flame IS NOT NULL\n",
        "    AND df.age_flame IS NOT NULL\n",
        "\n",
        "    -- Random Limit (Taking 50,000 Giants is sufficient for balancing)\n",
        "    AND g.random_index < 20000000\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    job_giant = Gaia.launch_job_async(query_giant)\n",
        "    df_giant_raw = job_giant.get_results().to_pandas()\n",
        "\n",
        "    # Initial Labeling\n",
        "    df_giant_raw['dataset_source'] = 'Gaia_DR3_Giant_Query'\n",
        "    df_giant_raw['evolutionary_phase'] = 'Giant'\n",
        "\n",
        "    print(f\"SUCCESS: Retrieved {len(df_giant_raw):,} Pure Giant Stars.\")\n",
        "    print(f\"   Evolstage Range: {df_giant_raw['evolstage_flame'].min()} - {df_giant_raw['evolstage_flame'].max()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to Query Giants: {e}\")\n",
        "    df_giant_raw = pd.DataFrame()"
      ],
      "metadata": {
        "id": "R2IfkDmlnat9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. WHITE DWARF DATA ACQUISITION (STANDARD ASTROQUERY METHOD)\n",
        "# ==============================================================================\n",
        "print(\"Fetching White Dwarf Catalog (Gentile Fusillo et al. 2021)...\")\n",
        "\n",
        "from astroquery.vizier import Vizier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Vizier Configuration: Request all columns [\"**\"]\n",
        "# Ensures critical columns like AgeH/MassH are not hidden\n",
        "v = Vizier(row_limit=50000, columns=[\"**\"])\n",
        "\n",
        "df_wd_raw = pd.DataFrame()\n",
        "\n",
        "try:\n",
        "    # 2. Retrieve tables from Catalog ID\n",
        "    catalogs = v.get_catalogs(\"J/MNRAS/508/3877\")\n",
        "    print(f\"   -> Found {len(catalogs)} tables.\")\n",
        "\n",
        "    # 3. Find the main table (the one with the most columns)\n",
        "    target_catalog = None\n",
        "    max_cols = 0\n",
        "\n",
        "    for i, table in enumerate(catalogs):\n",
        "        # Convert to Pandas first for safe column checking\n",
        "        temp_df = table.to_pandas()\n",
        "        print(f\"      - Table {i}: {len(temp_df)} rows, {len(temp_df.columns)} columns\")\n",
        "\n",
        "        if len(temp_df.columns) > max_cols:\n",
        "            max_cols = len(temp_df.columns)\n",
        "            target_catalog = temp_df\n",
        "\n",
        "    if target_catalog is not None:\n",
        "        df_wd_raw = target_catalog\n",
        "        print(f\"Selected table! Total: {len(df_wd_raw)} rows.\")\n",
        "\n",
        "        # Debug: Check for existence of Age/Mass columns\n",
        "        check_cols = [c for c in df_wd_raw.columns if 'Age' in c or 'Mass' in c]\n",
        "        print(f\"   -> Target columns detected: {check_cols[:5]}...\")\n",
        "    else:\n",
        "        raise ValueError(\"No valid table found.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to retrieve VizieR data: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. WD DATA HARMONIZATION (ROBUST CLEANING)\n",
        "# ==============================================================================\n",
        "def process_gentile_data_robust(df):\n",
        "    if df.empty: return df\n",
        "\n",
        "    data = df.copy()\n",
        "\n",
        "    # A. White Dwarf Probability Filter (PWD > 0.95)\n",
        "    pwd_cols = [c for c in data.columns if 'PWD' in c.upper()]\n",
        "    if pwd_cols:\n",
        "        col_pwd = pwd_cols[0]\n",
        "        print(f\"   -> Filtering PWD using column '{col_pwd}' > 0.95\")\n",
        "        data = data[data[col_pwd] > 0.95].copy()\n",
        "\n",
        "    # B. Column Name Mapping\n",
        "    mapping_candidates = {\n",
        "        'source_id': ['Source', 'EDR3', 'GaiaEDR3'],\n",
        "        'ra': ['RA_ICRS', 'RA'],\n",
        "        'dec': ['DE_ICRS', 'DE'],\n",
        "        'parallax': ['Plx'],\n",
        "        'parallax_error': ['e_Plx'],\n",
        "        'phot_g_mean_mag': ['Gmag'],\n",
        "        'phot_bp_mean_mag': ['BPmag'],\n",
        "        'phot_rp_mean_mag': ['RPmag'],\n",
        "        'mass_wd': ['MassH', 'Mass', 'mass'],       # Target 1 (Mass)\n",
        "        'age_wd_cooling': ['AgeH', 'Age', 'age'],   # Target 2 (Age)\n",
        "        'teff_wd': ['TeffH', 'Teff'],\n",
        "        'logg_wd': ['loggH', 'logg'],\n",
        "        'ruwe': ['RUWE']\n",
        "    }\n",
        "\n",
        "    rename_dict = {}\n",
        "    for target_col, candidates in mapping_candidates.items():\n",
        "        for cand in candidates:\n",
        "            if cand in data.columns:\n",
        "                rename_dict[cand] = target_col\n",
        "                break\n",
        "    data.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "    # B2. Safety Net for Age\n",
        "    if 'age_wd_cooling' not in data.columns:\n",
        "        fuzzy_age = [c for c in data.columns if \"age\" in c.lower()]\n",
        "        if fuzzy_age:\n",
        "            print(f\"   i Auto-recover age column: Using '{fuzzy_age[0]}'\")\n",
        "            data.rename(columns={fuzzy_age[0]: 'age_wd_cooling'}, inplace=True)\n",
        "\n",
        "    # C. Feature Engineering\n",
        "    if 'parallax' in data.columns and 'parallax_error' in data.columns:\n",
        "        data['parallax_over_error'] = data['parallax'] / data['parallax_error']\n",
        "    else:\n",
        "        data['parallax_over_error'] = np.nan\n",
        "\n",
        "    if 'phot_bp_mean_mag' in data.columns and 'phot_rp_mean_mag' in data.columns:\n",
        "        data['bp_rp0'] = data['phot_bp_mean_mag'] - data['phot_rp_mean_mag']\n",
        "    else:\n",
        "        data['bp_rp0'] = np.nan\n",
        "\n",
        "    if 'phot_g_mean_mag' in data.columns and 'parallax' in data.columns:\n",
        "        with np.errstate(divide='ignore'):\n",
        "            dist_pc = 1000.0 / data['parallax']\n",
        "            data['abs_G0'] = data['phot_g_mean_mag'] - 5 * np.log10(dist_pc) + 5\n",
        "    else:\n",
        "        data['abs_G0'] = np.nan\n",
        "\n",
        "    # D. Labeling\n",
        "    data['evolutionary_phase'] = 'White Dwarf'\n",
        "    data['dataset_source'] = 'GentileFusillo2021'\n",
        "\n",
        "    # E. Final Validation\n",
        "    if 'mass_wd' in data.columns and 'age_wd_cooling' in data.columns:\n",
        "        before = len(data)\n",
        "        data = data.dropna(subset=['mass_wd', 'age_wd_cooling'])\n",
        "        after = len(data)\n",
        "        print(f\"   -> COMPLETE Data (Mass & Age): {after} (Dropped {before-after})\")\n",
        "    else:\n",
        "        print(\"CRITICAL: Target Mass/Age columns missing!\")\n",
        "        print(\"   Available columns:\", data.columns.tolist())\n",
        "\n",
        "    return data\n",
        "\n",
        "# --- EXECUTE ---\n",
        "if not df_wd_raw.empty:\n",
        "    df_wd_clean = process_gentile_data_robust(df_wd_raw)\n",
        "    print(f\"Final WD Data Ready: {len(df_wd_clean)} rows.\")\n",
        "else:\n",
        "    print(\"Warning: Raw WD data is empty.\")"
      ],
      "metadata": {
        "id": "bfLXL-sgndpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. SCIENTIFIC PAPER CROSS-MATCH (DEBUGGED & FIXED)\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import re\n",
        "from astroquery.vizier import Vizier\n",
        "from astroquery.gaia import Gaia\n",
        "from astropy.coordinates import SkyCoord\n",
        "from astropy import units as u\n",
        "\n",
        "print(\"STARTING SCIENTIFIC LITERATURE-BASED CROSS-MATCH (REVISED)...\")\n",
        "\n",
        "# --- COORDINATE CONVERSION HELPER FUNCTION ---\n",
        "def clean_coord(val):\n",
        "    \"\"\"Forces conversion to float. Returns NaN if value is sexagesimal string (dropped later).\"\"\"\n",
        "    try:\n",
        "        return float(val)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. DEFINE TARGET CATALOGS\n",
        "# ------------------------------------------------------------------------------\n",
        "# We use IDs that are guaranteed to have data (Stable)\n",
        "target_papers = [\n",
        "    {\n",
        "        \"name\": \"Pantaleoni (ALS II)\",\n",
        "        \"id\": \"J/MNRAS/504/2968\",  # 2021 Version (Stable & Complete)\n",
        "        \"type\": \"Blue Supergiant/OB\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Messineo (RSG)\",\n",
        "        \"id\": \"J/AJ/158/20\",       # 2019 Version (Abundant & Stable)\n",
        "        \"type\": \"Red Supergiant\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Zhang (RSG)\",\n",
        "        \"id\": \"J/ApJ/889/33\",      # Zhang et al. 2020 (RSG Catalog)\n",
        "        \"type\": \"Red Supergiant\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. VIZIER DATA RETRIEVAL LOOP\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n[1/3] Fetching Coordinates from Vizier (Unlimited Rows)...\")\n",
        "\n",
        "# IMPORTANT: Set row_limit=-1 HERE to retrieve all available data\n",
        "v = Vizier(columns=[\"**\"], row_limit=-1)\n",
        "\n",
        "reference_stars = []\n",
        "\n",
        "for paper in target_papers:\n",
        "    print(f\"   -> Attempting catalog: {paper['name']} ({paper['id']})...\", end=\" \")\n",
        "\n",
        "    try:\n",
        "        cats = v.get_catalogs(paper['id'])\n",
        "\n",
        "        # Find the largest table (usually the main dataset)\n",
        "        df_res = pd.DataFrame()\n",
        "        max_rows = 0\n",
        "        for c in cats:\n",
        "            if len(c) > max_rows:\n",
        "                df_res = c.to_pandas()\n",
        "                max_rows = len(c)\n",
        "\n",
        "        if df_res.empty:\n",
        "            print(\"Empty.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Success ({len(df_res)} rows).\")\n",
        "\n",
        "        # --- AUTO-DETECT RA/DEC ---\n",
        "        cols = df_res.columns.tolist()\n",
        "\n",
        "        # Search for RA/DEC columns (prioritize decimal formats: RAJ2000, RA_ICRS, RAdeg)\n",
        "        c_ra = next((c for c in cols if c in ['RAJ2000', 'RA_ICRS', '_RA', 'RAdeg', 'RA']), None)\n",
        "        c_dec = next((c for c in cols if c in ['DEJ2000', 'DE_ICRS', '_DE', 'DEdeg', 'DE']), None)\n",
        "\n",
        "        # Search for Mass column (Specific to Pantaleoni)\n",
        "        c_mass = next((c for c in cols if c in ['Mass', 'Mini', 'M_evol', 'M']), None)\n",
        "\n",
        "        if c_ra and c_dec:\n",
        "            # Standardization\n",
        "            temp = pd.DataFrame()\n",
        "\n",
        "            # IMPORTANT: Force conversion to Float (Decimal)\n",
        "            # Vizier sometimes returns strings like \"18 20 30\" -> This causes SQL Errors\n",
        "            # We attempt simple conversion. If it fails, we use Astropy SkyCoord (slower but robust)\n",
        "\n",
        "            # Check sample data type\n",
        "            sample_val = df_res[c_ra].iloc[0]\n",
        "            if isinstance(sample_val, str) and \" \" in sample_val:\n",
        "                # This is hour/minute format (sexagesimal). Convert using SkyCoord\n",
        "                print(\"      Warning: String coordinate format detected. Converting...\", end=\" \")\n",
        "                coords = SkyCoord(df_res[c_ra].astype(str), df_res[c_dec].astype(str),\n",
        "                                  unit=(u.hourangle, u.deg), frame='icrs')\n",
        "                temp['ra_ref'] = coords.ra.deg\n",
        "                temp['dec_ref'] = coords.dec.deg\n",
        "                print(\"Done.\")\n",
        "            else:\n",
        "                # Assume decimal\n",
        "                temp['ra_ref'] = pd.to_numeric(df_res[c_ra], errors='coerce')\n",
        "                temp['dec_ref'] = pd.to_numeric(df_res[c_dec], errors='coerce')\n",
        "\n",
        "            temp['paper_source'] = paper['name']\n",
        "            temp['star_type'] = paper['type']\n",
        "\n",
        "            if c_mass:\n",
        "                temp['mass_ref'] = pd.to_numeric(df_res[c_mass], errors='coerce')\n",
        "            else:\n",
        "                temp['mass_ref'] = np.nan\n",
        "\n",
        "            # Drop rows where coordinate conversion failed\n",
        "            temp = temp.dropna(subset=['ra_ref', 'dec_ref'])\n",
        "            reference_stars.append(temp)\n",
        "        else:\n",
        "            print(f\"      Skip: Coordinate columns not recognized in {cols[:5]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# Concatenate\n",
        "if reference_stars:\n",
        "    df_refs = pd.concat(reference_stars, ignore_index=True)\n",
        "    print(f\"\\nTotal Valid Targets: {len(df_refs)} Stars.\")\n",
        "else:\n",
        "    print(\"Failed to aggregate reference stars.\")\n",
        "    df_refs = pd.DataFrame()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. GAIA DR3 CROSS-MATCH (WITH ERROR CHECKING)\n",
        "# ------------------------------------------------------------------------------\n",
        "if not df_refs.empty:\n",
        "    print(\"\\n[2/3] Cross-Matching with Gaia DR3 (Batch Query)...\")\n",
        "\n",
        "    gaia_results = []\n",
        "    targets = df_refs.to_dict('records')\n",
        "    batch_size = 50\n",
        "\n",
        "    for i in range(0, len(targets), batch_size):\n",
        "        batch = targets[i:i+batch_size]\n",
        "\n",
        "        # Query Construction\n",
        "        conditions = []\n",
        "        for t in batch:\n",
        "            # Ensure coordinates are valid floats\n",
        "            if pd.notna(t['ra_ref']) and pd.notna(t['dec_ref']):\n",
        "                cond = f\"1=CONTAINS(POINT('ICRS', ra, dec), CIRCLE('ICRS', {t['ra_ref']:.5f}, {t['dec_ref']:.5f}, 0.00028))\"\n",
        "                conditions.append(cond)\n",
        "\n",
        "        if not conditions: continue\n",
        "\n",
        "        where_clause = \" OR \".join(conditions)\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            source_id, ra, dec, parallax, parallax_over_error,\n",
        "            phot_g_mean_mag, phot_bp_mean_mag, phot_rp_mean_mag, bp_rp,\n",
        "            ruwe, phot_g_mean_flux_over_error,\n",
        "            teff_gspphot, logg_gspphot, mh_gspphot\n",
        "        FROM gaiadr3.gaia_source\n",
        "        WHERE ({where_clause})\n",
        "          AND phot_g_mean_mag < 19\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            job = Gaia.launch_job(query)\n",
        "            res = job.get_results().to_pandas()\n",
        "\n",
        "            if not res.empty:\n",
        "                # Reverse Match\n",
        "                for _, g_row in res.iterrows():\n",
        "                    best_match = None\n",
        "                    min_dist = 1.0\n",
        "\n",
        "                    for t_row in batch:\n",
        "                        d = np.sqrt((g_row['ra'] - t_row['ra_ref'])**2 + (g_row['dec'] - t_row['dec_ref'])**2)\n",
        "                        if d < min_dist:\n",
        "                            min_dist = d\n",
        "                            best_match = t_row\n",
        "\n",
        "                    if min_dist < 0.0004 and best_match:\n",
        "                        row_data = g_row.to_dict()\n",
        "                        row_data['paper_source'] = best_match['paper_source']\n",
        "                        row_data['star_type'] = best_match['star_type']\n",
        "                        row_data['mass_ref'] = best_match['mass_ref']\n",
        "\n",
        "                        # Evolutionary Phase Labeling (Int) for Neural Network\n",
        "                        # 0: Blue/OB, 1: Red SG\n",
        "                        row_data['label_class'] = 0 if 'Blue' in best_match['star_type'] else 1\n",
        "\n",
        "                        gaia_results.append(row_data)\n",
        "\n",
        "            print(f\"\\r   -> Batch {i//batch_size + 1}: Found {len(res)} candidates...\", end=\"\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # DISPLAY ERROR IF FAILED (Debugging purpose)\n",
        "            print(f\"\\n   Batch Error: {e}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"\\n\\nDone! Found {len(gaia_results)} valid matches.\")\n",
        "\n",
        "    # 4. SAVE\n",
        "    if gaia_results:\n",
        "        df_final = pd.DataFrame(gaia_results).drop_duplicates(subset='source_id')\n",
        "\n",
        "        # Calculate Abs Mag\n",
        "        df_final['G_abs'] = df_final['phot_g_mean_mag'] + 5 * np.log10(df_final['parallax']/1000.0) + 5\n",
        "\n",
        "        print(\"\\nFinal Statistics:\")\n",
        "        print(df_final['paper_source'].value_counts())\n",
        "\n",
        "        filename = \"gaia_scientific_supergiants_fixed.csv\"\n",
        "        df_final.to_csv(filename, index=False)\n",
        "        print(f\"File saved: {filename}\")\n",
        "else:\n",
        "    print(\"No data available.\")"
      ],
      "metadata": {
        "id": "NthPAzARnf0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. FINAL DATA MERGING (REVISED: 4 SOURCES INCLUDING GIANT QUERY)\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Merging 4 Datasets (Main + Giant + Scientific SG + WD)...\")\n",
        "\n",
        "frames = []\n",
        "\n",
        "# --- A. MAIN DATASET (Main Sequence/Subgiant) ---\n",
        "if 'df_utama' in locals() and not df_utama.empty:\n",
        "    df_u = df_utama.copy()\n",
        "    df_u['dataset_source'] = 'Gaia_DR3_Main'\n",
        "    # We focus this dataset on MS & Subgiant, leaving Giants to be handled by the specific query\n",
        "    # (Optional: Filter < 490 if strict cleanliness is needed, but deduplication handles it)\n",
        "\n",
        "    # Feature Calculation\n",
        "    df_u['bp_rp0'] = df_u['phot_bp_mean_mag'] - df_u['phot_rp_mean_mag']\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        df_u['abs_G0'] = df_u['phot_g_mean_mag'] - 5*np.log10(1000.0/df_u['parallax']) + 5\n",
        "\n",
        "    df_u['mass_wd'] = np.nan\n",
        "    df_u['age_wd_cooling'] = np.nan\n",
        "\n",
        "    frames.append(df_u)\n",
        "    print(f\"   -> [Main] {len(df_u):,} stars ready.\")\n",
        "\n",
        "# --- B. SPECIFIC GIANT DATASET (Additional Query) ---\n",
        "if 'df_giant_raw' in locals() and not df_giant_raw.empty:\n",
        "    df_g = df_giant_raw.copy()\n",
        "    df_g['dataset_source'] = 'Gaia_DR3_Giant_Query'\n",
        "\n",
        "    # Feature Calculation\n",
        "    df_g['bp_rp0'] = df_g['phot_bp_mean_mag'] - df_g['phot_rp_mean_mag']\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        df_g['abs_G0'] = df_g['phot_g_mean_mag'] - 5*np.log10(1000.0/df_g['parallax']) + 5\n",
        "\n",
        "    df_g['mass_wd'] = np.nan\n",
        "    df_g['age_wd_cooling'] = np.nan\n",
        "\n",
        "    frames.append(df_g)\n",
        "    print(f\"   -> [Giant Query] {len(df_g):,} stars ready.\")\n",
        "\n",
        "# --- C. SUPERGIANT DATASET (SCIENTIFIC - High Priority) ---\n",
        "try:\n",
        "    if 'df_final' in locals() and 'paper_source' in df_final.columns:\n",
        "        df_sg_sci = df_final.copy()\n",
        "    else:\n",
        "        df_sg_sci = pd.read_csv(\"gaia_scientific_supergiants_fixed.csv\")\n",
        "\n",
        "    df_sg_sci['dataset_source'] = 'Scientific_Paper_SG'\n",
        "\n",
        "    # Dummy Code for SG\n",
        "    df_sg_sci['evolstage_flame'] = 400\n",
        "\n",
        "    if 'mass_ref' in df_sg_sci.columns:\n",
        "        df_sg_sci.rename(columns={'mass_ref': 'mass_flame'}, inplace=True)\n",
        "\n",
        "    if 'bp_rp0' not in df_sg_sci.columns:\n",
        "        if 'bp_rp' in df_sg_sci.columns:\n",
        "             df_sg_sci['bp_rp0'] = df_sg_sci['bp_rp']\n",
        "        else:\n",
        "             df_sg_sci['bp_rp0'] = df_sg_sci['phot_bp_mean_mag'] - df_sg_sci['phot_rp_mean_mag']\n",
        "\n",
        "    if 'abs_G0' not in df_sg_sci.columns and 'G_abs' in df_sg_sci.columns:\n",
        "         df_sg_sci['abs_G0'] = df_sg_sci['G_abs']\n",
        "\n",
        "    if 'age_flame' not in df_sg_sci.columns:\n",
        "        df_sg_sci['age_flame'] = np.nan\n",
        "\n",
        "    frames.append(df_sg_sci)\n",
        "    print(f\"   -> [Scientific SG] {len(df_sg_sci):,} stars (Gold Standard).\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Scientific SG dataset not found ({e}).\")\n",
        "\n",
        "# --- D. WHITE DWARF DATASET (Gentile Fusillo) ---\n",
        "if 'df_wd_clean' in locals() and not df_wd_clean.empty:\n",
        "    df_w = df_wd_clean.copy()\n",
        "\n",
        "    if 'RUWE' in df_w.columns:\n",
        "        df_w.rename(columns={'RUWE': 'ruwe'}, inplace=True)\n",
        "\n",
        "    df_w['mass_flame'] = np.nan\n",
        "    df_w['age_flame'] = np.nan\n",
        "    df_w['evolstage_flame'] = 500\n",
        "\n",
        "    frames.append(df_w)\n",
        "    print(f\"   -> [White Dwarf] {len(df_w):,} stars ready.\")\n",
        "\n",
        "\n",
        "# --- E. EXECUTE MERGE & INTELLIGENT DEDUPLICATION ---\n",
        "if frames:\n",
        "    df_gabungan = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    # Priority Logic (Lower number = Higher Priority)\n",
        "    # 1. Scientific Paper (Human Verified) -> Most Valid\n",
        "    # 2. White Dwarf (Specific Catalog)\n",
        "    # 3. Giant Query (Specific Catalog)\n",
        "    # 4. Main Dataset (General)\n",
        "    source_priority = {\n",
        "        'Scientific_Paper_SG': 0,\n",
        "        'Vizier_Pantaleoni2021': 0,\n",
        "        'Vizier_Hohle2010': 0,\n",
        "        'GentileFusillo2021': 1,\n",
        "        'Gaia_DR3_Giant_Query': 2,\n",
        "        'Gaia_DR3_Main': 3\n",
        "    }\n",
        "\n",
        "    df_gabungan['prio'] = df_gabungan['dataset_source'].map(source_priority).fillna(99)\n",
        "    df_gabungan.sort_values('prio', inplace=True)\n",
        "\n",
        "    # Remove Duplicates based on Source ID\n",
        "    before_dedup = len(df_gabungan)\n",
        "    df_gabungan.drop_duplicates(subset=['source_id'], keep='first', inplace=True)\n",
        "    after_dedup = len(df_gabungan)\n",
        "\n",
        "    print(f\"\\nDeduplication Complete: {before_dedup - after_dedup} duplicates removed.\")\n",
        "\n",
        "    # Final Quality Filter\n",
        "    df_gabungan = df_gabungan[\n",
        "        (df_gabungan['parallax'] > 0) &\n",
        "        (df_gabungan['bp_rp0'].notnull()) &\n",
        "        (df_gabungan['abs_G0'].notnull())\n",
        "    ].copy()\n",
        "\n",
        "    # Shuffle Data\n",
        "    df_gabungan = df_gabungan.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\nFINAL MASTER DATASET READY!\")\n",
        "    print(f\"Total Rows: {len(df_gabungan):,}\")\n",
        "    print(\"-\" * 30)\n",
        "    print(\"Data Source Distribution:\")\n",
        "    print(df_gabungan['dataset_source'].value_counts())\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Check for True Massive Stars (> 10 M_sun)\n",
        "    monsters = len(df_gabungan[df_gabungan['mass_flame'] > 10.0])\n",
        "    print(f\"True Massive Stars (> 10 M_sun): {monsters}\")\n",
        "\n",
        "    # Save to Master CSV\n",
        "    df_gabungan.to_csv(\"FINAL_MASTER_DATASET.csv\", index=False)\n",
        "\n",
        "else:\n",
        "    print(\"Failed to create df_gabungan.\")"
      ],
      "metadata": {
        "id": "L7M6DYQynkxw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}