{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt614k4hvAMN"
      },
      "outputs": [],
      "source": [
        "# 13. REGRESSION MODEL ARCHITECTURES (REVISED: PRE-NORM & EXTERNAL COMPILE)\n",
        "# ==============================================================================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# --- CUSTOM METRIC: R-SQUARED (R2) ---\n",
        "def r2_keras(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom R2-score metric for Keras monitoring.\n",
        "    R2 = 1 - (SS_res / SS_tot)\n",
        "    \"\"\"\n",
        "    SS_res =  tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
        "    return (1 - SS_res/(SS_tot + tf.keras.backend.epsilon()))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODEL A: PRE-NORM RESNET-MLP REGRESSOR\n",
        "# Revision: Post-Norm -> Pre-Norm, Removed Compile\n",
        "# ------------------------------------------------------------------------------\n",
        "def build_mlp_regressor(input_dim, width=512, depth=6, dropout_rate=0.1):\n",
        "    inputs = layers.Input(shape=(input_dim,), name=\"input_features\")\n",
        "\n",
        "    # 1. Linear Projection (To match block dimensions)\n",
        "    x = layers.Dense(width, activation='linear')(inputs)\n",
        "\n",
        "    # 2. Residual Blocks (Pre-Norm Style)\n",
        "    #\n",
        "    for i in range(depth):\n",
        "        shortcut = x # Save Identity\n",
        "\n",
        "        # Normalize FIRST (Pre-Norm)\n",
        "        x_norm = layers.LayerNormalization()(x)\n",
        "\n",
        "        # Transformation Branch (Dense -> Dropout -> Dense)\n",
        "        branch = layers.Dense(width, activation='gelu')(x_norm)\n",
        "        branch = layers.Dropout(dropout_rate)(branch)\n",
        "        branch = layers.Dense(width, activation='linear')(branch)\n",
        "\n",
        "        # Merge (Add)\n",
        "        x = layers.Add()([shortcut, branch])\n",
        "\n",
        "    # 3. Output Head\n",
        "    x = layers.LayerNormalization()(x) # Final Norm is mandatory for Pre-Norm\n",
        "    x = layers.Dense(64, activation='gelu')(x)\n",
        "\n",
        "    # Output: 2 Neurons (Mass, Age) - Linear Activation for Regression\n",
        "    outputs = layers.Dense(2, activation='linear', name='reg_output')(x)\n",
        "\n",
        "    # NOTE: model.compile REMOVED to allow external scheduling\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"ResNet_MLP_Regressor\")\n",
        "    return model\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODEL B: PRE-NORM FT-TRANSFORMER REGRESSOR\n",
        "# Revision: Pooling -> Flatten, Post-Norm -> Pre-Norm\n",
        "# ------------------------------------------------------------------------------\n",
        "def build_transformer_regressor(input_dim, embed_dim=64, num_heads=4, num_blocks=3, dropout=0.1):\n",
        "    inputs = layers.Input(shape=(input_dim,), name=\"input_features\")\n",
        "\n",
        "    # 1. Feature Tokenizer\n",
        "    # Converts each feature into a learned embedding vector\n",
        "    x = layers.Reshape((input_dim, 1))(inputs)\n",
        "    x = layers.Conv1D(filters=embed_dim, kernel_size=1, activation=None)(x)\n",
        "\n",
        "    # 2. Transformer Blocks (Pre-Norm Style)\n",
        "    #\n",
        "    for i in range(num_blocks):\n",
        "        # --- Attention Sub-layer ---\n",
        "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x) # Pre-Norm\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads,\n",
        "            dropout=dropout\n",
        "        )(x_norm, x_norm)\n",
        "        x = layers.Add()([x, attn_output])\n",
        "\n",
        "        # --- Feed Forward Sub-layer ---\n",
        "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x) # Pre-Norm\n",
        "        ffn = keras.Sequential([\n",
        "            layers.Dense(embed_dim * 2, activation='gelu'),\n",
        "            layers.Dropout(dropout),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        ffn_output = ffn(x_norm)\n",
        "        x = layers.Add()([x, ffn_output])\n",
        "\n",
        "    # 3. Prediction Head\n",
        "    # Using Flatten instead of GlobalAveragePooling to preserve feature-specific information\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x) # Final Norm\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    x = layers.Dense(64, activation='gelu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    # Output: 2 Neurons (Mass, Age)\n",
        "    outputs = layers.Dense(2, activation='linear', name='reg_output')(x)\n",
        "\n",
        "    # NOTE: model.compile REMOVED\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"Transformer_Regressor\")\n",
        "    return model\n",
        "\n",
        "print(\"Regression Architectures Ready (Pre-Norm Version).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. MLP TRAINING & EVALUATION (REVISED: PRE-NORM + WARMUP SCHEDULER)\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers, losses, callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "import os\n",
        "\n",
        "# Set Scientific Plot Style\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['grid.alpha'] = 0.3\n",
        "\n",
        "# --- 1. DATA SETUP ---\n",
        "print(\"Preparing Data for MLP Regressor...\")\n",
        "# Load Data\n",
        "try:\n",
        "    df_reg = pd.read_parquet(\"df_reg_flame_final.parquet\")\n",
        "    print(f\"   -> Data loaded: {len(df_reg):,} rows.\")\n",
        "except:\n",
        "    print(\"Error: Parquet file not found. Please run Preprocessing (Section 8) first!\")\n",
        "    df_reg = pd.DataFrame()\n",
        "\n",
        "if not df_reg.empty:\n",
        "    # --- CONFIG INPUT FEATURES ---\n",
        "    # Ensuring teff_gspphot is included for better regression accuracy\n",
        "    input_cols = [\n",
        "        'bp_rp0', 'bp_g', 'g_rp',\n",
        "        'abs_G0',\n",
        "        'parallax', 'ruwe',\n",
        "        'l_norm',\n",
        "        'teff_gspphot'  # [IMPORTANT] Effective Temperature Feature\n",
        "    ]\n",
        "\n",
        "    # Safety Check for missing columns\n",
        "    missing_cols = [c for c in input_cols if c not in df_reg.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"Warning: The following columns are missing: {missing_cols}\")\n",
        "        input_cols = [c for c in input_cols if c in df_reg.columns]\n",
        "\n",
        "    print(f\"   -> Using {len(input_cols)} Input Features.\")\n",
        "\n",
        "    X = df_reg[input_cols].values\n",
        "    # We predict Log-Mass and Log-Age to stabilize training\n",
        "    y = df_reg[['log_mass', 'log_age']].values\n",
        "\n",
        "    # Split Data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Scaling\n",
        "    # QuantileTransformer is ideal for MLP to Gaussianize input features\n",
        "    print(\"   -> Scaling Data...\")\n",
        "    scaler_X = QuantileTransformer(output_distribution='normal', random_state=42)\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "    scaler_y = StandardScaler()\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "    # --- HYPERPARAMETER & SCHEDULER ---\n",
        "    BATCH_SIZE = 2048\n",
        "    EPOCHS = 50\n",
        "\n",
        "    # [REVISION] Define Warmup Scheduler (Cosine Decay)\n",
        "    # This replaces ReduceLROnPlateau for better stability in deep networks\n",
        "    steps_per_epoch = len(X_train_scaled) // BATCH_SIZE\n",
        "    total_steps = steps_per_epoch * EPOCHS\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=1e-5,      # Start small (Warmup)\n",
        "        decay_steps=total_steps,\n",
        "        alpha=0.01,                      # Decay to 1% of max LR\n",
        "        warmup_target=1e-3,              # Max LR for MLP\n",
        "        warmup_steps=int(0.1 * total_steps) # 10% Warmup duration\n",
        "    )\n",
        "\n",
        "    # --- 3. BUILD & COMPILE ---\n",
        "    print(f\"\\nBuilding & Compiling MLP Model (Pre-Norm)...\")\n",
        "\n",
        "    if 'build_mlp_regressor' in locals():\n",
        "        model_mlp = build_mlp_regressor(\n",
        "            input_dim=X_train.shape[1],\n",
        "            width=512,\n",
        "            depth=6,\n",
        "            dropout_rate=0.1\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Error: 'build_mlp_regressor' function is not defined.\")\n",
        "\n",
        "    # [REVISION] Manual Compile with Huber Loss\n",
        "    optimizer = optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4)\n",
        "\n",
        "    model_mlp.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=losses.Huber(delta=1.0), # Huber loss is robust against outliers in astronomical data\n",
        "        metrics=['mae', 'mse', r2_keras],\n",
        "        jit_compile=True\n",
        "    )\n",
        "\n",
        "    # --- 4. TRAINING LOOP ---\n",
        "    callbacks_list = [\n",
        "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    history_mlp = model_mlp.fit(\n",
        "        X_train_scaled, y_train_scaled,\n",
        "        validation_data=(X_test_scaled, y_test_scaled),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=callbacks_list,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # --- 5. PHYSICAL EVALUATION (CRITICAL STEP) ---\n",
        "    print(\"\\nEvaluating Results in Physical Units...\")\n",
        "\n",
        "    # 1. Predict (in Log Normal Scale)\n",
        "    preds_scaled = model_mlp.predict(X_test_scaled, verbose=0)\n",
        "\n",
        "    # 2. Inverse Standard Scaler (Back to Log Scale)\n",
        "    preds_log = scaler_y.inverse_transform(preds_scaled)\n",
        "    y_true_log = scaler_y.inverse_transform(y_test_scaled)\n",
        "\n",
        "    # 3. CONVERT TO PHYSICAL UNITS (10^x)\n",
        "    pred_mass_phys = 10 ** preds_log[:, 0]\n",
        "    true_mass_phys = 10 ** y_true_log[:, 0]\n",
        "\n",
        "    pred_age_phys  = 10 ** preds_log[:, 1]\n",
        "    true_age_phys  = 10 ** y_true_log[:, 1]\n",
        "\n",
        "    # 4. Calculate Physical Errors\n",
        "    mae_mass = mean_absolute_error(true_mass_phys, pred_mass_phys)\n",
        "    mae_age = mean_absolute_error(true_age_phys, pred_age_phys)\n",
        "\n",
        "    r2_mass = r2_score(true_mass_phys, pred_mass_phys)\n",
        "    r2_age = r2_score(true_age_phys, pred_age_phys)\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"PHYSICAL EVALUATION RESULTS (Pre-Norm MLP):\")\n",
        "    print(f\"   -> Mass ($M_\\odot$): MAE = {mae_mass:.3f} $M_\\odot$ | R2 = {r2_mass:.4f}\")\n",
        "    print(f\"   -> Age (Gyr)    : MAE = {mae_age:.3f} Gyr | R2 = {r2_age:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # --- VISUALIZATION (PHYSICAL UNITS) ---\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(14, 12))\n",
        "    plt.suptitle(\"MLP Evaluation: Physical Units ($M_{\\odot}$ & Gyr)\", fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Scatter Plot: Mass\n",
        "    ax[0,0].scatter(true_mass_phys, pred_mass_phys, s=2, alpha=0.3, color='tab:blue')\n",
        "    min_m, max_m = true_mass_phys.min(), true_mass_phys.max()\n",
        "    ax[0,0].plot([min_m, max_m], [min_m, max_m], 'r--', lw=2)\n",
        "    ax[0,0].set_title(f\"Mass: Predicted vs Actual ($R^2={r2_mass:.3f}$)\")\n",
        "    ax[0,0].set_xlabel(\"Actual Mass ($M_{\\odot}$)\")\n",
        "    ax[0,0].set_ylabel(\"Predicted Mass ($M_{\\odot}$)\")\n",
        "    ax[0,0].set_xscale('log'); ax[0,0].set_yscale('log')\n",
        "\n",
        "    # Scatter Plot: Age\n",
        "    ax[0,1].scatter(true_age_phys, pred_age_phys, s=2, alpha=0.3, color='tab:orange')\n",
        "    min_a, max_a = true_age_phys.min(), true_age_phys.max()\n",
        "    ax[0,1].plot([min_a, max_a], [min_a, max_a], 'r--', lw=2)\n",
        "    ax[0,1].set_title(f\"Age: Predicted vs Actual ($R^2={r2_age:.3f}$)\")\n",
        "    ax[0,1].set_xlabel(\"Actual Age (Gyr)\")\n",
        "    ax[0,1].set_ylabel(\"Predicted Age (Gyr)\")\n",
        "    ax[0,1].set_xscale('log'); ax[0,1].set_yscale('log')\n",
        "\n",
        "    # Residual Plot: Mass\n",
        "    res_mass = pred_mass_phys - true_mass_phys\n",
        "    ax[1,0].scatter(true_mass_phys, res_mass, s=2, alpha=0.3, color='purple')\n",
        "    ax[1,0].axhline(0, color='black', linestyle='--')\n",
        "    ax[1,0].set_title(f\"Mass Residuals (MAE: {mae_mass:.3f} $M_\\odot$)\")\n",
        "    ax[1,0].set_xlabel(\"Actual Mass ($M_{\\odot}$)\")\n",
        "    ax[1,0].set_ylabel(\"Error ($Pred - True$)\")\n",
        "    ax[1,0].set_xscale('log')\n",
        "\n",
        "    # Residual Plot: Age\n",
        "    res_age = pred_age_phys - true_age_phys\n",
        "    ax[1,1].scatter(true_age_phys, res_age, s=2, alpha=0.3, color='green')\n",
        "    ax[1,1].axhline(0, color='black', linestyle='--')\n",
        "    ax[1,1].set_title(f\"Age Residuals (MAE: {mae_age:.3f} Gyr)\")\n",
        "    ax[1,1].set_xlabel(\"Actual Age (Gyr)\")\n",
        "    ax[1,1].set_ylabel(\"Error ($Pred - True$)\")\n",
        "    ax[1,1].set_xscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('mlp_physical_evaluation.png', dpi=300)\n",
        "    plt.show()\n",
        "    print(\"Plot saved: mlp_physical_evaluation.png\")\n",
        "\n",
        "    # Save Model\n",
        "    model_mlp.save('best_mlp_regressor_full_physics.keras')\n",
        "    print(\"Model saved: best_mlp_regressor_full_physics.keras\")"
      ],
      "metadata": {
        "id": "PUYpElIRvQWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. TRANSFORMER REGRESSION ARCHITECTURE (V2: PRE-NORM STABILITY)\n",
        "# ==============================================================================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers, losses\n",
        "\n",
        "# --- CUSTOM METRIC: R-SQUARED (R2) ---\n",
        "def r2_keras(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom R2 metric for Keras.\n",
        "    Formula: 1 - (SS_res / SS_tot)\n",
        "    \"\"\"\n",
        "    SS_res =  tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
        "    return (1 - SS_res/(SS_tot + tf.keras.backend.epsilon()))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MODEL DEFINITION: PRE-NORM FT-TRANSFORMER\n",
        "# ------------------------------------------------------------------------------\n",
        "def build_transformer_regressor_v2(input_dim, embed_dim=64, num_heads=8, num_blocks=4, dropout=0.1):\n",
        "    \"\"\"\n",
        "    Builds a Pre-Norm Transformer for Regression.\n",
        "\n",
        "    Key Architectural Choices:\n",
        "    1. Pre-Norm: LayerNormalization is applied BEFORE Attention/FFN.\n",
        "       This creates a 'gradient highway' for better stability in deep networks (Standard in GPT-3/PaLM).\n",
        "    2. Flattening: Instead of GlobalAveragePooling, we flatten the features\n",
        "       to preserve the distinct information of each physical parameter.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(input_dim,), name=\"input_features\")\n",
        "\n",
        "    # 1. Feature Tokenizer\n",
        "    # Projects scalar features into an embedding space\n",
        "    x = layers.Reshape((input_dim, 1))(inputs)\n",
        "    x = layers.Conv1D(filters=embed_dim, kernel_size=1, activation=None)(x)\n",
        "\n",
        "    # 2. Transformer Blocks (Pre-Norm Architecture)\n",
        "    #\n",
        "    for i in range(num_blocks):\n",
        "        # --- Sub-layer 1: Multi-Head Attention ---\n",
        "        # Normalize FIRST\n",
        "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads,\n",
        "            dropout=dropout\n",
        "        )(x_norm, x_norm)\n",
        "\n",
        "        # Skip Connection (Add)\n",
        "        x = layers.Add()([x, attn_output])\n",
        "\n",
        "        # --- Sub-layer 2: Feed Forward Network (FFN) ---\n",
        "        # Normalize FIRST\n",
        "        x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        ffn = keras.Sequential([\n",
        "            layers.Dense(embed_dim * 2, activation='gelu'), # Expand dim 2x\n",
        "            layers.Dropout(dropout),\n",
        "            layers.Dense(embed_dim), # Project back\n",
        "        ])\n",
        "        ffn_output = ffn(x_norm)\n",
        "\n",
        "        # Skip Connection (Add)\n",
        "        x = layers.Add()([x, ffn_output])\n",
        "\n",
        "    # 3. Output Head (Flattening Strategy)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x) # Final Norm\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    # Deep Regressor Head\n",
        "    x = layers.Dense(128, activation='gelu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(64, activation='gelu')(x)\n",
        "\n",
        "    # Output: 2 targets (Mass, Age)\n",
        "    outputs = layers.Dense(2, activation='linear', name='reg_output')(x)\n",
        "\n",
        "    # NOTE: model.compile is intentionally omitted to allow external scheduling\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"PreNorm_Transformer_V2\")\n",
        "    return model\n",
        "\n",
        "print(\"Transformer V2 Architecture (Pre-Norm) Ready.\")"
      ],
      "metadata": {
        "id": "59bLDWDhvgqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. TRANSFORMER V2 TRAINING (REVISED: SAVE PLOTS & LEARNING CURVES)\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers, losses\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Ensure data exists (from Section 14)\n",
        "if 'X_train_scaled' in locals():\n",
        "\n",
        "    # --- [NEW] 0. FEATURE CORRELATION MATRIX (OPTIONAL CHECK) ---\n",
        "    # Plot correlation only if the dataframe is still in memory\n",
        "    if 'df_reg' in locals() and 'input_cols' in locals():\n",
        "        print(\"Generating Feature Correlation Matrix...\")\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # Merge inputs and targets for correlation analysis\n",
        "        cols_to_plot = input_cols + ['log_mass', 'log_age']\n",
        "        # Filter strictly existing columns to prevent errors\n",
        "        cols_to_plot = [c for c in cols_to_plot if c in df_reg.columns]\n",
        "\n",
        "        df_corr = df_reg[cols_to_plot].corr()\n",
        "        sns.heatmap(df_corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
        "        plt.title(\"Feature & Target Correlation Matrix (Transformer Data)\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('transformer_v2_feature_correlation.png', dpi=300)\n",
        "        plt.show()\n",
        "        print(\"Plot saved: transformer_v2_feature_correlation.png\")\n",
        "\n",
        "    # --- 1. CONFIGURATION ---\n",
        "    BATCH_SIZE = 512\n",
        "    EPOCHS = 50\n",
        "\n",
        "    # --- 2. LEARNING RATE SCHEDULER (Cosine Decay + Warmup) ---\n",
        "    # Strategy: Start from 0, ramp up to Max, then decay slowly\n",
        "    #\n",
        "    total_steps = (len(X_train_scaled) // BATCH_SIZE) * EPOCHS\n",
        "    warmup_steps = int(0.1 * total_steps) # 10% steps for warmup\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=1e-5,      # Warmup start (very small)\n",
        "        decay_steps=total_steps,\n",
        "        alpha=0.01,                      # Final LR = 1% of Max LR\n",
        "        warmup_target=1e-3,              # Peak LR\n",
        "        warmup_steps=warmup_steps\n",
        "    )\n",
        "\n",
        "    # Optimizer with Weight Decay (AdamW) for better regularization\n",
        "    optimizer = optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4)\n",
        "\n",
        "    # --- 3. BUILD & COMPILE ---\n",
        "    print(\"\\nBuilding Transformer V2 (Pre-Norm)...\")\n",
        "    # Verify function definition\n",
        "    if 'build_transformer_regressor_v2' in locals():\n",
        "        model_trans_v2 = build_transformer_regressor_v2(\n",
        "            input_dim=X_train_scaled.shape[1],\n",
        "            embed_dim=64,\n",
        "            num_heads=8,\n",
        "            num_blocks=4,\n",
        "            dropout=0.1\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Error: 'build_transformer_regressor_v2' function not defined.\")\n",
        "\n",
        "    model_trans_v2.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=losses.Huber(delta=1.0),\n",
        "        metrics=['mae', 'mse', r2_keras],\n",
        "        jit_compile=True\n",
        "    )\n",
        "\n",
        "    # --- 4. TRAINING ---\n",
        "    print(\"Starting Training with Warmup Scheduler...\")\n",
        "    callbacks_list = [\n",
        "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    history_trans_v2 = model_trans_v2.fit(\n",
        "        X_train_scaled, y_train_scaled,\n",
        "        validation_data=(X_test_scaled, y_test_scaled),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=callbacks_list,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # --- 5. EVALUATION & VISUALIZATION ---\n",
        "    print(\"\\nEvaluating Transformer V2...\")\n",
        "\n",
        "    # A. Learning Curves (Critical for checking Warmup effect)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_trans_v2.history['loss'], label='Train Loss')\n",
        "    plt.plot(history_trans_v2.history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss Curve (Huber)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_trans_v2.history['r2_keras'], label='Train R2')\n",
        "    plt.plot(history_trans_v2.history['val_r2_keras'], label='Val R2')\n",
        "    plt.title('R2 Score Curve')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('transformer_v2_learning_curves.png', dpi=300)\n",
        "    plt.show()\n",
        "    print(\"Plot saved: transformer_v2_learning_curves.png\")\n",
        "\n",
        "    # B. Prediction Plots\n",
        "    preds_scaled = model_trans_v2.predict(X_test_scaled, verbose=0)\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    y_true = scaler_y.inverse_transform(y_test_scaled)\n",
        "\n",
        "    r2_mass = r2_score(y_true[:,0], preds[:,0])\n",
        "    r2_age = r2_score(y_true[:,1], preds[:,1])\n",
        "\n",
        "    print(f\"   -> R2 Score (Mass): {r2_mass:.4f}\")\n",
        "    print(f\"   -> R2 Score (Age) : {r2_age:.4f}\")\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    plt.suptitle(f\"Transformer V2 Results (Log Scale)\", fontsize=16)\n",
        "\n",
        "    # Mass (Log Scale)\n",
        "    ax[0].scatter(y_true[:,0], preds[:,0], s=1, alpha=0.3, color='tab:blue')\n",
        "    ax[0].plot([y_true[:,0].min(), y_true[:,0].max()], [y_true[:,0].min(), y_true[:,0].max()], 'r--')\n",
        "    ax[0].set_title(f\"Log Mass ($R^2={r2_mass:.3f}$)\")\n",
        "    ax[0].set_xlabel(\"True Log Mass\"); ax[0].set_ylabel(\"Pred Log Mass\")\n",
        "\n",
        "    # Age (Log Scale)\n",
        "    ax[1].scatter(y_true[:,1], preds[:,1], s=1, alpha=0.3, color='tab:orange')\n",
        "    ax[1].plot([y_true[:,1].min(), y_true[:,1].max()], [y_true[:,1].min(), y_true[:,1].max()], 'r--')\n",
        "    ax[1].set_title(f\"Log Age ($R^2={r2_age:.3f}$)\")\n",
        "    ax[1].set_xlabel(\"True Log Age\"); ax[1].set_ylabel(\"Pred Log Age\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('transformer_v2_prediction_analysis.png', dpi=300)\n",
        "    plt.show()\n",
        "    print(\"Plot saved: transformer_v2_prediction_analysis.png\")\n",
        "\n",
        "    # Save Final Model\n",
        "    model_trans_v2.save('best_transformer_v2_regressor.keras')\n",
        "    print(\"Model saved: best_transformer_v2_regressor.keras\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: Please run Section 14 first to load and scale data.\")"
      ],
      "metadata": {
        "id": "p0y81fQnvugt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}